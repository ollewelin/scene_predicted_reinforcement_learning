************************************************************************************************
Normal mode is selected by user Next scene network will be used with frame future f-2,f-1,f0,f+1 
skip_scene_predictor_only_for_benchmarking = 0
************************************************************************************************
Next f+2 predictor network as well on th way.. not finnish yet ,,,
Parmeters settings :
next_scene_hid_nodes_L1 = 200
next_scene_hid_nodes_L2 = 200
next_scene_hid_nodes_L3 = 200
policy_net_hid_nodes_L1 = 85
policy_net_hid_nodes_L2 = 25
policy_net_hid_nodes_L3 = 10
next_scene_fc_net.learning_rate = 0.001
next_scene_fc_net.momentum = 0.1
policy_fc_net.learning_rate = 0.001
policy_fc_net.momentum = 0.2
g_replay_size = 200
derating_epsilon = 0.0005
gamma_decay = 0.85
target_policy_on_next_action_selection = 1
target_policy_use_f_p_2_pixel_from_predict_net = 1
target_off_set_to_output_itself = 1
skip_traning_next_scenen = 0
policy_fc_net.use_dropouts =0
policy_fc_net.dropout_proportion = 0
retrain_policy_net = 1
************************************************************************************************
Normal mode is selected by user Next scene network will be used with frame future f-2,f-1,f0,f+1 
skip_scene_predictor_only_for_benchmarking = 0
************************************************************************************************
Next f+2 predictor network as well on th way.. not finnish yet ,,,
Parmeters settings :
next_scene_hid_nodes_L1 = 200
next_scene_hid_nodes_L2 = 200
next_scene_hid_nodes_L3 = 200
policy_net_hid_nodes_L1 = 85
policy_net_hid_nodes_L2 = 25
policy_net_hid_nodes_L3 = 10
next_scene_fc_net.learning_rate = 0.001
next_scene_fc_net.momentum = 0.1
policy_fc_net.learning_rate = 0.001
policy_fc_net.momentum = 0.2
g_replay_size = 20
derating_epsilon = 5e-05
gamma_decay = 0.85
target_policy_on_next_action_selection = 1
target_policy_use_f_p_2_pixel_from_predict_net = 1
target_off_set_to_output_itself = 1
skip_traning_next_scenen = 0
policy_fc_net.use_dropouts =0
policy_fc_net.dropout_proportion = 0
retrain_policy_net = 1
epoch = 1 epsilon = 0.93 retrain_policy net = 0 Policy net Loss = 216.012
************************************************************************************************
Normal mode is selected by user Next scene network will be used with frame future f-2,f-1,f0,f+1 
skip_scene_predictor_only_for_benchmarking = 0
************************************************************************************************
Next f+2 predictor network as well on th way.. not finnish yet ,,,
Parmeters settings :
next_scene_hid_nodes_L1 = 200
next_scene_hid_nodes_L2 = 200
next_scene_hid_nodes_L3 = 200
policy_net_hid_nodes_L1 = 85
policy_net_hid_nodes_L2 = 25
policy_net_hid_nodes_L3 = 10
next_scene_fc_net.learning_rate = 0.001
next_scene_fc_net.momentum = 0.1
policy_fc_net.learning_rate = 0.001
policy_fc_net.momentum = 0.2
g_replay_size = 20
derating_epsilon = 5e-05
gamma_decay = 0.85
target_policy_on_next_action_selection = 1
target_policy_use_f_p_2_pixel_from_predict_net = 1
target_off_set_to_output_itself = 1
skip_traning_next_scenen = 0
policy_fc_net.use_dropouts =0
policy_fc_net.dropout_proportion = 0
retrain_policy_net = 1
epoch = 1 epsilon = 0.93 retrain_policy net = 0 Policy net Loss = 191.615
epoch = 2 epsilon = 0.91 retrain_policy net = 0 Policy net Loss = 245.21
epoch = 3 epsilon = 0.89 retrain_policy net = 0 Policy net Loss = 181.029
epoch = 4 epsilon = 0.88995 retrain_policy net = 0 Policy net Loss = 208.979
epoch = 5 epsilon = 0.8899 retrain_policy net = 0 Policy net Loss = 211.54
epoch = 6 epsilon = 0.88985 retrain_policy net = 0 Policy net Loss = 217.886
epoch = 7 epsilon = 0.8898 retrain_policy net = 0 Policy net Loss = 169.935
epoch = 8 epsilon = 0.88975 retrain_policy net = 0 Policy net Loss = 193.897
epoch = 9 epsilon = 0.8897 retrain_policy net = 0 Policy net Loss = 199.012
epoch = 10 epsilon = 0.88965 retrain_policy net = 0 Policy net Loss = 211.66
epoch = 11 epsilon = 0.8896 retrain_policy net = 0 Policy net Loss = 220.85
epoch = 12 epsilon = 0.88955 retrain_policy net = 0 Policy net Loss = 159.698
epoch = 13 epsilon = 0.8895 retrain_policy net = 0 Policy net Loss = 159.119
epoch = 14 epsilon = 0.88945 retrain_policy net = 0 Policy net Loss = 202.53
epoch = 15 epsilon = 0.8894 retrain_policy net = 0 Policy net Loss = 177.001
************************************************************************************************
Normal mode is selected by user Next scene network will be used with frame future f-2,f-1,f0,f+1 
skip_scene_predictor_only_for_benchmarking = 0
************************************************************************************************
Next f+2 predictor network as well on th way.. not finnish yet ,,,
Parmeters settings :
next_scene_hid_nodes_L1 = 200
next_scene_hid_nodes_L2 = 200
next_scene_hid_nodes_L3 = 200
policy_net_hid_nodes_L1 = 85
policy_net_hid_nodes_L2 = 25
policy_net_hid_nodes_L3 = 10
next_scene_fc_net.learning_rate = 0.001
next_scene_fc_net.momentum = 0.1
policy_fc_net.learning_rate = 0.001
policy_fc_net.momentum = 0.2
g_replay_size = 20
derating_epsilon = 5e-05
gamma_decay = 0.85
target_policy_on_next_action_selection = 1
target_policy_use_f_p_2_pixel_from_predict_net = 1
target_off_set_to_output_itself = 1
skip_traning_next_scenen = 0
policy_fc_net.use_dropouts =0
policy_fc_net.dropout_proportion = 0
retrain_policy_net = 1
epoch = 1 epsilon = 0.93 retrain_policy net = 0 Policy net Loss = 208.511
epoch = 2 epsilon = 0.91 retrain_policy net = 0 Policy net Loss = 206.054
epoch = 3 epsilon = 0.89 retrain_policy net = 0 Policy net Loss = 198.807
epoch = 4 epsilon = 0.88995 retrain_policy net = 0 Policy net Loss = 180.71
epoch = 5 epsilon = 0.8899 retrain_policy net = 0 Policy net Loss = 172.093
epoch = 6 epsilon = 0.88985 retrain_policy net = 0 Policy net Loss = 176.358
epoch = 7 epsilon = 0.8898 retrain_policy net = 0 Policy net Loss = 192.514
************************************************************************************************
Normal mode is selected by user Next scene network will be used with frame future f-2,f-1,f0,f+1 
skip_scene_predictor_only_for_benchmarking = 0
************************************************************************************************
Next f+2 predictor network as well on th way.. not finnish yet ,,,
Parmeters settings :
next_scene_hid_nodes_L1 = 200
next_scene_hid_nodes_L2 = 200
next_scene_hid_nodes_L3 = 200
policy_net_hid_nodes_L1 = 85
policy_net_hid_nodes_L2 = 25
policy_net_hid_nodes_L3 = 10
next_scene_fc_net.learning_rate = 0.001
next_scene_fc_net.momentum = 0.1
policy_fc_net.learning_rate = 0.001
policy_fc_net.momentum = 0.2
g_replay_size = 20
derating_epsilon = 5e-05
gamma_decay = 0.85
target_policy_on_next_action_selection = 1
target_policy_use_f_p_2_pixel_from_predict_net = 1
target_off_set_to_output_itself = 1
skip_traning_next_scenen = 0
policy_fc_net.use_dropouts =0
policy_fc_net.dropout_proportion = 0
retrain_policy_net = 1
epoch = 1 epsilon = 0.93 retrain_policy net = 0 Policy net Loss = 207.057
epoch = 2 epsilon = 0.91 retrain_policy net = 0 Policy net Loss = 192.296
epoch = 3 epsilon = 0.89 retrain_policy net = 0 Policy net Loss = 160.504
epoch = 4 epsilon = 0.88995 retrain_policy net = 0 Policy net Loss = 173.839
epoch = 5 epsilon = 0.8899 retrain_policy net = 0 Policy net Loss = 214.859
epoch = 6 epsilon = 0.88985 retrain_policy net = 0 Policy net Loss = 197.009
epoch = 7 epsilon = 0.8898 retrain_policy net = 0 Policy net Loss = 188.067
epoch = 8 epsilon = 0.88975 retrain_policy net = 0 Policy net Loss = 184.528
epoch = 9 epsilon = 0.8897 retrain_policy net = 0 Policy net Loss = 177.999
epoch = 10 epsilon = 0.88965 retrain_policy net = 0 Policy net Loss = 206.867
epoch = 11 epsilon = 0.8896 retrain_policy net = 0 Policy net Loss = 226.552
epoch = 12 epsilon = 0.88955 retrain_policy net = 0 Policy net Loss = 195.224
epoch = 13 epsilon = 0.8895 retrain_policy net = 0 Policy net Loss = 168.314
epoch = 14 epsilon = 0.88945 retrain_policy net = 0 Policy net Loss = 212.835
************************************************************************************************
Normal mode is selected by user Next scene network will be used with frame future f-2,f-1,f0,f+1 
skip_scene_predictor_only_for_benchmarking = 0
************************************************************************************************
Next f+2 predictor network as well on th way.. not finnish yet ,,,
Parmeters settings :
next_scene_hid_nodes_L1 = 200
next_scene_hid_nodes_L2 = 200
next_scene_hid_nodes_L3 = 200
policy_net_hid_nodes_L1 = 85
policy_net_hid_nodes_L2 = 25
policy_net_hid_nodes_L3 = 10
next_scene_fc_net.learning_rate = 0.001
next_scene_fc_net.momentum = 0.1
policy_fc_net.learning_rate = 0.001
policy_fc_net.momentum = 0.2
g_replay_size = 20
derating_epsilon = 5e-05
gamma_decay = 0.85
target_policy_on_next_action_selection = 1
target_policy_use_f_p_2_pixel_from_predict_net = 1
target_off_set_to_output_itself = 1
skip_traning_next_scenen = 0
policy_fc_net.use_dropouts =0
policy_fc_net.dropout_proportion = 0
retrain_policy_net = 1
epoch = 1 epsilon = 0.93 retrain_policy net = 0 Policy net Loss = 185.289
epoch = 2 epsilon = 0.91 retrain_policy net = 0 Policy net Loss = 170.014
epoch = 3 epsilon = 0.89 retrain_policy net = 0 Policy net Loss = 189.913
epoch = 4 epsilon = 0.88995 retrain_policy net = 0 Policy net Loss = 228.058
epoch = 5 epsilon = 0.8899 retrain_policy net = 0 Policy net Loss = 176.398
epoch = 6 epsilon = 0.88985 retrain_policy net = 0 Policy net Loss = 191.396
************************************************************************************************
Normal mode is selected by user Next scene network will be used with frame future f-2,f-1,f0,f+1 
skip_scene_predictor_only_for_benchmarking = 0
************************************************************************************************
Next f+2 predictor network as well on th way.. not finnish yet ,,,
Parmeters settings :
next_scene_hid_nodes_L1 = 200
next_scene_hid_nodes_L2 = 200
next_scene_hid_nodes_L3 = 200
policy_net_hid_nodes_L1 = 85
policy_net_hid_nodes_L2 = 25
policy_net_hid_nodes_L3 = 10
next_scene_fc_net.learning_rate = 0.001
next_scene_fc_net.momentum = 0.1
policy_fc_net.learning_rate = 0.001
policy_fc_net.momentum = 0.2
g_replay_size = 20
derating_epsilon = 5e-05
gamma_decay = 0.85
target_policy_on_next_action_selection = 1
target_policy_use_f_p_2_pixel_from_predict_net = 1
target_off_set_to_output_itself = 1
skip_traning_next_scenen = 0
policy_fc_net.use_dropouts =0
policy_fc_net.dropout_proportion = 0
retrain_policy_net = 1
************************************************************************************************
Normal mode is selected by user Next scene network will be used with frame future f-2,f-1,f0,f+1 
skip_scene_predictor_only_for_benchmarking = 0
************************************************************************************************
Next f+2 predictor network as well on th way.. not finnish yet ,,,
Parmeters settings :
next_scene_hid_nodes_L1 = 200
next_scene_hid_nodes_L2 = 200
next_scene_hid_nodes_L3 = 200
policy_net_hid_nodes_L1 = 85
policy_net_hid_nodes_L2 = 25
policy_net_hid_nodes_L3 = 10
next_scene_fc_net.learning_rate = 0.001
next_scene_fc_net.momentum = 0.1
policy_fc_net.learning_rate = 0.001
policy_fc_net.momentum = 0.2
g_replay_size = 20
derating_epsilon = 5e-05
gamma_decay = 0.85
target_policy_on_next_action_selection = 1
target_policy_use_f_p_2_pixel_from_predict_net = 1
target_off_set_to_output_itself = 1
skip_traning_next_scenen = 0
policy_fc_net.use_dropouts =0
policy_fc_net.dropout_proportion = 0
retrain_policy_net = 1
epoch = 1 epsilon = 0.93 retrain_policy net = 0 Policy net Loss = 220.94
************************************************************************************************
Normal mode is selected by user Next scene network will be used with frame future f-2,f-1,f0,f+1 
skip_scene_predictor_only_for_benchmarking = 0
************************************************************************************************
Next f+2 predictor network as well on th way.. not finnish yet ,,,
Parmeters settings :
next_scene_hid_nodes_L1 = 200
next_scene_hid_nodes_L2 = 200
next_scene_hid_nodes_L3 = 200
policy_net_hid_nodes_L1 = 85
policy_net_hid_nodes_L2 = 25
policy_net_hid_nodes_L3 = 10
next_scene_fc_net.learning_rate = 0.001
next_scene_fc_net.momentum = 0.1
policy_fc_net.learning_rate = 0.001
policy_fc_net.momentum = 0.2
g_replay_size = 20
derating_epsilon = 5e-05
gamma_decay = 0.85
target_policy_on_next_action_selection = 1
target_policy_use_f_p_2_pixel_from_predict_net = 1
target_off_set_to_output_itself = 1
skip_traning_next_scenen = 0
policy_fc_net.use_dropouts =0
policy_fc_net.dropout_proportion = 0
retrain_policy_net = 1
epoch = 1 epsilon = 0.93 retrain_policy net = 0 Policy net Loss = 189.247
epoch = 2 epsilon = 0.91 retrain_policy net = 0 Policy net Loss = 221.198
epoch = 3 epsilon = 0.89 retrain_policy net = 0 Policy net Loss = 205.897
epoch = 4 epsilon = 0.88995 retrain_policy net = 0 Policy net Loss = 195.885
epoch = 5 epsilon = 0.8899 retrain_policy net = 0 Policy net Loss = 197.709
epoch = 6 epsilon = 0.88985 retrain_policy net = 0 Policy net Loss = 186.88
epoch = 7 epsilon = 0.8898 retrain_policy net = 0 Policy net Loss = 216.117
epoch = 8 epsilon = 0.88975 retrain_policy net = 0 Policy net Loss = 186.086
epoch = 9 epsilon = 0.8897 retrain_policy net = 0 Policy net Loss = 196.69
epoch = 10 epsilon = 0.88965 retrain_policy net = 0 Policy net Loss = 195.687
epoch = 11 epsilon = 0.8896 retrain_policy net = 0 Policy net Loss = 216.773
epoch = 12 epsilon = 0.88955 retrain_policy net = 0 Policy net Loss = 210.043
epoch = 13 epsilon = 0.8895 retrain_policy net = 0 Policy net Loss = 205.297
epoch = 14 epsilon = 0.88945 retrain_policy net = 0 Policy net Loss = 201.998
epoch = 15 epsilon = 0.8894 retrain_policy net = 0 Policy net Loss = 175.441
epoch = 16 epsilon = 0.88935 retrain_policy net = 0 Policy net Loss = 160.274
epoch = 17 epsilon = 0.8893 retrain_policy net = 0 Policy net Loss = 206.776
epoch = 18 epsilon = 0.88925 retrain_policy net = 0 Policy net Loss = 202.007
epoch = 19 epsilon = 0.8892 retrain_policy net = 0 Policy net Loss = 210.211
epoch = 20 epsilon = 0.88915 retrain_policy net = 0 Policy net Loss = 227.35
epoch = 21 epsilon = 0.8891 retrain_policy net = 0 Policy net Loss = 206.817
epoch = 22 epsilon = 0.88905 retrain_policy net = 0 Policy net Loss = 195.352
epoch = 23 epsilon = 0.889 retrain_policy net = 0 Policy net Loss = 200.669
epoch = 24 epsilon = 0.88895 retrain_policy net = 0 Policy net Loss = 203.224
************************************************************************************************
Normal mode is selected by user Next scene network will be used with frame future f-2,f-1,f0,f+1 
skip_scene_predictor_only_for_benchmarking = 0
************************************************************************************************
Next f+2 predictor network as well on th way.. not finnish yet ,,,
Parmeters settings :
next_scene_hid_nodes_L1 = 200
next_scene_hid_nodes_L2 = 200
next_scene_hid_nodes_L3 = 200
policy_net_hid_nodes_L1 = 85
policy_net_hid_nodes_L2 = 25
policy_net_hid_nodes_L3 = 10
next_scene_fc_net.learning_rate = 0.001
next_scene_fc_net.momentum = 0.1
policy_fc_net.learning_rate = 0.001
policy_fc_net.momentum = 0.2
g_replay_size = 20
derating_epsilon = 5e-05
gamma_decay = 0.85
target_policy_on_next_action_selection = 1
target_policy_use_f_p_2_pixel_from_predict_net = 1
target_off_set_to_output_itself = 1
skip_traning_next_scenen = 0
policy_fc_net.use_dropouts =0
policy_fc_net.dropout_proportion = 0
retrain_policy_net = 1
epoch = 1 epsilon = 0.93 retrain_policy net = 0 Policy net Loss = 218.857
epoch = 2 epsilon = 0.91 retrain_policy net = 0 Policy net Loss = 190.85
************************************************************************************************
Normal mode is selected by user Next scene network will be used with frame future f-2,f-1,f0,f+1 
skip_scene_predictor_only_for_benchmarking = 0
************************************************************************************************
Next f+2 predictor network as well on th way.. not finnish yet ,,,
Parmeters settings :
next_scene_hid_nodes_L1 = 200
next_scene_hid_nodes_L2 = 200
next_scene_hid_nodes_L3 = 200
policy_net_hid_nodes_L1 = 85
policy_net_hid_nodes_L2 = 25
policy_net_hid_nodes_L3 = 10
next_scene_fc_net.learning_rate = 0.001
next_scene_fc_net.momentum = 0.1
policy_fc_net.learning_rate = 0.001
policy_fc_net.momentum = 0.2
g_replay_size = 20
derating_epsilon = 5e-05
gamma_decay = 0.85
target_policy_on_next_action_selection = 1
target_policy_use_f_p_2_pixel_from_predict_net = 1
target_off_set_to_output_itself = 1
skip_traning_next_scenen = 0
policy_fc_net.use_dropouts =0
policy_fc_net.dropout_proportion = 0
retrain_policy_net = 1
************************************************************************************************
Normal mode is selected by user Next scene network will be used with frame future f-2,f-1,f0,f+1 
skip_scene_predictor_only_for_benchmarking = 0
************************************************************************************************
Next f+2 predictor network as well on th way.. not finnish yet ,,,
Parmeters settings :
next_scene_hid_nodes_L1 = 200
next_scene_hid_nodes_L2 = 200
next_scene_hid_nodes_L3 = 200
policy_net_hid_nodes_L1 = 85
policy_net_hid_nodes_L2 = 25
policy_net_hid_nodes_L3 = 10
next_scene_fc_net.learning_rate = 0.001
next_scene_fc_net.momentum = 0.1
policy_fc_net.learning_rate = 0.001
policy_fc_net.momentum = 0.2
g_replay_size = 20
derating_epsilon = 5e-05
gamma_decay = 0.85
target_policy_on_next_action_selection = 1
target_policy_use_f_p_2_pixel_from_predict_net = 1
target_off_set_to_output_itself = 1
skip_traning_next_scenen = 0
policy_fc_net.use_dropouts =0
policy_fc_net.dropout_proportion = 0
retrain_policy_net = 1
epoch = 1 epsilon = 0.93 retrain_policy net = 0 Policy net Loss = 202.719
epoch = 2 epsilon = 0.91 retrain_policy net = 0 Policy net Loss = 203.4
epoch = 3 epsilon = 0.89 retrain_policy net = 0 Policy net Loss = 198.515
epoch = 4 epsilon = 0.88995 retrain_policy net = 0 Policy net Loss = 176.122
epoch = 5 epsilon = 0.8899 retrain_policy net = 0 Policy net Loss = 210.909
epoch = 6 epsilon = 0.88985 retrain_policy net = 0 Policy net Loss = 180.477
epoch = 7 epsilon = 0.8898 retrain_policy net = 0 Policy net Loss = 198.163
epoch = 8 epsilon = 0.88975 retrain_policy net = 0 Policy net Loss = 225.959
epoch = 9 epsilon = 0.8897 retrain_policy net = 0 Policy net Loss = 179.093
epoch = 10 epsilon = 0.88965 retrain_policy net = 0 Policy net Loss = 164.066
epoch = 11 epsilon = 0.8896 retrain_policy net = 0 Policy net Loss = 177.521
epoch = 12 epsilon = 0.88955 retrain_policy net = 0 Policy net Loss = 185.418
epoch = 13 epsilon = 0.8895 retrain_policy net = 0 Policy net Loss = 207.797
epoch = 14 epsilon = 0.88945 retrain_policy net = 0 Policy net Loss = 186.419
epoch = 15 epsilon = 0.8894 retrain_policy net = 0 Policy net Loss = 214.272
epoch = 16 epsilon = 0.88935 retrain_policy net = 0 Policy net Loss = 163.868
epoch = 17 epsilon = 0.8893 retrain_policy net = 0 Policy net Loss = 213.801
epoch = 18 epsilon = 0.88925 retrain_policy net = 0 Policy net Loss = 175.548
epoch = 19 epsilon = 0.8892 retrain_policy net = 0 Policy net Loss = 181.011
epoch = 20 epsilon = 0.88915 retrain_policy net = 0 Policy net Loss = 208.052
epoch = 21 epsilon = 0.8891 retrain_policy net = 0 Policy net Loss = 208.082
epoch = 22 epsilon = 0.88905 retrain_policy net = 0 Policy net Loss = 169.287
epoch = 23 epsilon = 0.889 retrain_policy net = 0 Policy net Loss = 169.36
epoch = 24 epsilon = 0.88895 retrain_policy net = 0 Policy net Loss = 168.24
epoch = 25 epsilon = 0.8889 retrain_policy net = 0 Policy net Loss = 212.287
epoch = 26 epsilon = 0.88885 retrain_policy net = 0 Policy net Loss = 139.975
epoch = 27 epsilon = 0.8888 retrain_policy net = 0 Policy net Loss = 198.391
epoch = 28 epsilon = 0.88875 retrain_policy net = 0 Policy net Loss = 183.715
epoch = 29 epsilon = 0.8887 retrain_policy net = 0 Policy net Loss = 174.451
epoch = 30 epsilon = 0.88865 retrain_policy net = 0 Policy net Loss = 210.252
************************************************************************************************
Normal mode is selected by user Next scene network will be used with frame future f-2,f-1,f0,f+1 
skip_scene_predictor_only_for_benchmarking = 0
************************************************************************************************
Next f+2 predictor network as well on th way.. not finnish yet ,,,
Parmeters settings :
next_scene_hid_nodes_L1 = 200
next_scene_hid_nodes_L2 = 200
next_scene_hid_nodes_L3 = 200
next_F2_scene_hid_nodes_L1 = 200
next_F2_scene_hid_nodes_L2 = 200
next_F2_scene_hid_nodes_L3 = 200
policy_net_hid_nodes_L1 = 85
policy_net_hid_nodes_L2 = 25
policy_net_hid_nodes_L3 = 10
next_scene_fc_net.learning_rate = 0.001
next_scene_fc_net.momentum = 0.1
next_F2_scene_fc_net.learning_rate = 0.001
next_F2_scene_fc_net.momentum = 0.1
policy_fc_net.learning_rate = 0.001
policy_fc_net.momentum = 0.2
g_replay_size = 20
derating_epsilon = 5e-05
gamma_decay = 0.85
target_policy_on_next_action_selection = 1
target_policy_use_f_p_2_pixel_from_predict_net = 1
target_off_set_to_output_itself = 1
skip_traning_next_scenen = 0
policy_fc_net.use_dropouts =0
policy_fc_net.dropout_proportion = 0
retrain_policy_net = 1
epoch = 1 epsilon = 0.93 retrain_policy net = 0 Policy net Loss = 197.411
************************************************************************************************
Normal mode is selected by user Next scene network will be used with frame future f-2,f-1,f0,f+1 
skip_scene_predictor_only_for_benchmarking = 0
************************************************************************************************
Next f+2 predictor network as well on th way.. not finnish yet ,,,
Parmeters settings :
next_scene_hid_nodes_L1 = 200
next_scene_hid_nodes_L2 = 200
next_scene_hid_nodes_L3 = 200
next_F2_scene_hid_nodes_L1 = 200
next_F2_scene_hid_nodes_L2 = 200
next_F2_scene_hid_nodes_L3 = 200
policy_net_hid_nodes_L1 = 85
policy_net_hid_nodes_L2 = 25
policy_net_hid_nodes_L3 = 10
next_scene_fc_net.learning_rate = 0.001
next_scene_fc_net.momentum = 0.1
next_F2_scene_fc_net.learning_rate = 0.001
next_F2_scene_fc_net.momentum = 0.1
policy_fc_net.learning_rate = 0.001
policy_fc_net.momentum = 0.2
g_replay_size = 20
derating_epsilon = 5e-05
gamma_decay = 0.85
target_policy_on_next_action_selection = 1
target_policy_use_f_p_2_pixel_from_predict_net = 1
target_off_set_to_output_itself = 1
skip_traning_next_scenen = 0
policy_fc_net.use_dropouts =0
policy_fc_net.dropout_proportion = 0
retrain_policy_net = 1
epoch = 1 epsilon = 0.93 retrain_policy net = 0 Policy net Loss = 171.305
epoch = 2 epsilon = 0.91 retrain_policy net = 0 Policy net Loss = 138.786
epoch = 3 epsilon = 0.89 retrain_policy net = 0 Policy net Loss = 215.496
epoch = 4 epsilon = 0.88995 retrain_policy net = 0 Policy net Loss = 150.705
epoch = 5 epsilon = 0.8899 retrain_policy net = 0 Policy net Loss = 184.383
************************************************************************************************
Normal mode is selected by user Next scene network will be used with frame future f-2,f-1,f0,f+1 
skip_scene_predictor_only_for_benchmarking = 0
************************************************************************************************
Next f+2 predictor network as well on th way.. not finnish yet ,,,
Parmeters settings :
next_scene_hid_nodes_L1 = 200
next_scene_hid_nodes_L2 = 200
next_scene_hid_nodes_L3 = 200
next_F2_scene_hid_nodes_L1 = 200
next_F2_scene_hid_nodes_L2 = 200
next_F2_scene_hid_nodes_L3 = 200
policy_net_hid_nodes_L1 = 85
policy_net_hid_nodes_L2 = 25
policy_net_hid_nodes_L3 = 10
next_scene_fc_net.learning_rate = 0.001
next_scene_fc_net.momentum = 0.1
next_F2_scene_fc_net.learning_rate = 0.001
next_F2_scene_fc_net.momentum = 0.1
policy_fc_net.learning_rate = 0.001
policy_fc_net.momentum = 0.2
g_replay_size = 20
derating_epsilon = 5e-05
gamma_decay = 0.85
target_policy_on_next_action_selection = 1
target_policy_use_f_p_2_pixel_from_predict_net = 1
target_off_set_to_output_itself = 1
skip_traning_next_scenen = 0
policy_fc_net.use_dropouts =0
policy_fc_net.dropout_proportion = 0
retrain_policy_net = 1
epoch = 1 epsilon = 0.93 retrain_policy net = 0 Policy net Loss = 195.756
epoch = 2 epsilon = 0.91 retrain_policy net = 0 Policy net Loss = 208.6
epoch = 3 epsilon = 0.89 retrain_policy net = 0 Policy net Loss = 194.033
epoch = 4 epsilon = 0.88995 retrain_policy net = 0 Policy net Loss = 212.159
epoch = 5 epsilon = 0.8899 retrain_policy net = 0 Policy net Loss = 224.741
epoch = 6 epsilon = 0.88985 retrain_policy net = 0 Policy net Loss = 187.404
epoch = 7 epsilon = 0.8898 retrain_policy net = 0 Policy net Loss = 164.143
epoch = 8 epsilon = 0.88975 retrain_policy net = 0 Policy net Loss = 187.897
epoch = 9 epsilon = 0.8897 retrain_policy net = 0 Policy net Loss = 174.355
epoch = 10 epsilon = 0.88965 retrain_policy net = 0 Policy net Loss = 170.354
epoch = 11 epsilon = 0.8896 retrain_policy net = 0 Policy net Loss = 198.758
epoch = 12 epsilon = 0.88955 retrain_policy net = 0 Policy net Loss = 174.238
epoch = 13 epsilon = 0.8895 retrain_policy net = 0 Policy net Loss = 187.213
epoch = 14 epsilon = 0.88945 retrain_policy net = 0 Policy net Loss = 183.504
epoch = 15 epsilon = 0.8894 retrain_policy net = 0 Policy net Loss = 165.268
epoch = 16 epsilon = 0.88935 retrain_policy net = 0 Policy net Loss = 218.477
epoch = 17 epsilon = 0.8893 retrain_policy net = 0 Policy net Loss = 169.309
epoch = 18 epsilon = 0.88925 retrain_policy net = 0 Policy net Loss = 209.416
epoch = 19 epsilon = 0.8892 retrain_policy net = 0 Policy net Loss = 193.692
epoch = 20 epsilon = 0.88915 retrain_policy net = 0 Policy net Loss = 190.167
epoch = 21 epsilon = 0.8891 retrain_policy net = 0 Policy net Loss = 183.077
epoch = 22 epsilon = 0.88905 retrain_policy net = 0 Policy net Loss = 190.102
epoch = 23 epsilon = 0.889 retrain_policy net = 0 Policy net Loss = 163.601
epoch = 24 epsilon = 0.88895 retrain_policy net = 0 Policy net Loss = 175.82
epoch = 25 epsilon = 0.8889 retrain_policy net = 0 Policy net Loss = 182.21
epoch = 26 epsilon = 0.88885 retrain_policy net = 0 Policy net Loss = 207.692
epoch = 27 epsilon = 0.8888 retrain_policy net = 0 Policy net Loss = 167.369
epoch = 28 epsilon = 0.88875 retrain_policy net = 0 Policy net Loss = 190.995
epoch = 29 epsilon = 0.8887 retrain_policy net = 0 Policy net Loss = 162.718
epoch = 30 epsilon = 0.88865 retrain_policy net = 0 Policy net Loss = 164.016
epoch = 31 epsilon = 0.8886 retrain_policy net = 0 Policy net Loss = 174.963
epoch = 32 epsilon = 0.88855 retrain_policy net = 0 Policy net Loss = 204.484
epoch = 33 epsilon = 0.8885 retrain_policy net = 0 Policy net Loss = 160.222
epoch = 34 epsilon = 0.88845 retrain_policy net = 0 Policy net Loss = 196.305
epoch = 35 epsilon = 0.8884 retrain_policy net = 0 Policy net Loss = 156.957
epoch = 36 epsilon = 0.88835 retrain_policy net = 0 Policy net Loss = 181.614
epoch = 37 epsilon = 0.8883 retrain_policy net = 0 Policy net Loss = 180.896
epoch = 38 epsilon = 0.88825 retrain_policy net = 0 Policy net Loss = 190.465
epoch = 39 epsilon = 0.8882 retrain_policy net = 0 Policy net Loss = 169.884
epoch = 40 epsilon = 0.88815 retrain_policy net = 0 Policy net Loss = 175.531
epoch = 41 epsilon = 0.8881 retrain_policy net = 0 Policy net Loss = 182.733
epoch = 42 epsilon = 0.88805 retrain_policy net = 0 Policy net Loss = 199.25
epoch = 43 epsilon = 0.888 retrain_policy net = 0 Policy net Loss = 155.112
epoch = 44 epsilon = 0.88795 retrain_policy net = 0 Policy net Loss = 203.201
epoch = 45 epsilon = 0.8879 retrain_policy net = 0 Policy net Loss = 184.071
epoch = 46 epsilon = 0.88785 retrain_policy net = 0 Policy net Loss = 174.686
epoch = 47 epsilon = 0.8878 retrain_policy net = 0 Policy net Loss = 138.537
epoch = 48 epsilon = 0.88775 retrain_policy net = 0 Policy net Loss = 184.708
epoch = 49 epsilon = 0.8877 retrain_policy net = 0 Policy net Loss = 177.081
epoch = 50 epsilon = 0.88765 retrain_policy net = 0 Policy net Loss = 197.894
epoch = 51 epsilon = 0.8876 retrain_policy net = 0 Policy net Loss = 193.404
epoch = 52 epsilon = 0.88755 retrain_policy net = 0 Policy net Loss = 189.342
epoch = 53 epsilon = 0.8875 retrain_policy net = 0 Policy net Loss = 171.151
epoch = 54 epsilon = 0.88745 retrain_policy net = 0 Policy net Loss = 181.17
epoch = 55 epsilon = 0.8874 retrain_policy net = 0 Policy net Loss = 207.654
epoch = 56 epsilon = 0.88735 retrain_policy net = 0 Policy net Loss = 157.403
epoch = 57 epsilon = 0.8873 retrain_policy net = 0 Policy net Loss = 172.985
epoch = 58 epsilon = 0.88725 retrain_policy net = 0 Policy net Loss = 173.131
epoch = 59 epsilon = 0.8872 retrain_policy net = 0 Policy net Loss = 190.332
epoch = 60 epsilon = 0.88715 retrain_policy net = 0 Policy net Loss = 220.394
epoch = 61 epsilon = 0.8871 retrain_policy net = 0 Policy net Loss = 205.732
epoch = 62 epsilon = 0.88705 retrain_policy net = 0 Policy net Loss = 189.212
epoch = 63 epsilon = 0.887 retrain_policy net = 0 Policy net Loss = 165.821
epoch = 64 epsilon = 0.88695 retrain_policy net = 0 Policy net Loss = 137.695
epoch = 65 epsilon = 0.8869 retrain_policy net = 0 Policy net Loss = 194.154
epoch = 66 epsilon = 0.88685 retrain_policy net = 0 Policy net Loss = 199.441
epoch = 67 epsilon = 0.8868 retrain_policy net = 0 Policy net Loss = 200.65
epoch = 68 epsilon = 0.88675 retrain_policy net = 0 Policy net Loss = 212.36
epoch = 69 epsilon = 0.8867 retrain_policy net = 0 Policy net Loss = 150.062
epoch = 70 epsilon = 0.88665 retrain_policy net = 0 Policy net Loss = 167.796
epoch = 71 epsilon = 0.8866 retrain_policy net = 0 Policy net Loss = 185.493
epoch = 72 epsilon = 0.88655 retrain_policy net = 0 Policy net Loss = 177.731
epoch = 73 epsilon = 0.8865 retrain_policy net = 0 Policy net Loss = 156.46
epoch = 74 epsilon = 0.88645 retrain_policy net = 0 Policy net Loss = 167.708
epoch = 75 epsilon = 0.8864 retrain_policy net = 0 Policy net Loss = 178.097
epoch = 76 epsilon = 0.88635 retrain_policy net = 0 Policy net Loss = 164.367
epoch = 77 epsilon = 0.8863 retrain_policy net = 0 Policy net Loss = 209.369
epoch = 78 epsilon = 0.88625 retrain_policy net = 0 Policy net Loss = 152.188
epoch = 79 epsilon = 0.8862 retrain_policy net = 0 Policy net Loss = 206.179
epoch = 80 epsilon = 0.88615 retrain_policy net = 0 Policy net Loss = 157.235
epoch = 81 epsilon = 0.8861 retrain_policy net = 0 Policy net Loss = 169.404
epoch = 82 epsilon = 0.88605 retrain_policy net = 0 Policy net Loss = 228.83
epoch = 83 epsilon = 0.886 retrain_policy net = 0 Policy net Loss = 176.23
epoch = 84 epsilon = 0.88595 retrain_policy net = 0 Policy net Loss = 200.498
epoch = 85 epsilon = 0.8859 retrain_policy net = 0 Policy net Loss = 168.361
epoch = 86 epsilon = 0.88585 retrain_policy net = 0 Policy net Loss = 169.178
epoch = 87 epsilon = 0.8858 retrain_policy net = 0 Policy net Loss = 207.641
epoch = 88 epsilon = 0.88575 retrain_policy net = 0 Policy net Loss = 204.755
epoch = 89 epsilon = 0.8857 retrain_policy net = 0 Policy net Loss = 182.632
epoch = 90 epsilon = 0.88565 retrain_policy net = 0 Policy net Loss = 203.746
epoch = 91 epsilon = 0.8856 retrain_policy net = 0 Policy net Loss = 206.434
epoch = 92 epsilon = 0.88555 retrain_policy net = 0 Policy net Loss = 172.135
epoch = 93 epsilon = 0.8855 retrain_policy net = 0 Policy net Loss = 201.024
epoch = 94 epsilon = 0.88545 retrain_policy net = 0 Policy net Loss = 201.852
epoch = 95 epsilon = 0.8854 retrain_policy net = 0 Policy net Loss = 194.655
epoch = 96 epsilon = 0.88535 retrain_policy net = 0 Policy net Loss = 143.75
epoch = 97 epsilon = 0.8853 retrain_policy net = 0 Policy net Loss = 223.714
epoch = 98 epsilon = 0.88525 retrain_policy net = 0 Policy net Loss = 151.829
epoch = 99 epsilon = 0.8852 retrain_policy net = 0 Policy net Loss = 198.005
epoch = 100 epsilon = 0.88515 retrain_policy net = 0 Policy net Loss = 211.178
epoch = 101 epsilon = 0.8851 retrain_policy net = 0 Policy net Loss = 168.963
epoch = 102 epsilon = 0.88505 retrain_policy net = 0 Policy net Loss = 176.98
epoch = 103 epsilon = 0.885 retrain_policy net = 0 Policy net Loss = 194.56
epoch = 104 epsilon = 0.88495 retrain_policy net = 0 Policy net Loss = 159.95
epoch = 105 epsilon = 0.8849 retrain_policy net = 0 Policy net Loss = 182.314
epoch = 106 epsilon = 0.88485 retrain_policy net = 0 Policy net Loss = 191.369
epoch = 107 epsilon = 0.8848 retrain_policy net = 0 Policy net Loss = 148.906
epoch = 108 epsilon = 0.88475 retrain_policy net = 0 Policy net Loss = 201.334
epoch = 109 epsilon = 0.8847 retrain_policy net = 0 Policy net Loss = 174.33
epoch = 110 epsilon = 0.88465 retrain_policy net = 0 Policy net Loss = 202.354
epoch = 111 epsilon = 0.8846 retrain_policy net = 0 Policy net Loss = 177.667
epoch = 112 epsilon = 0.88455 retrain_policy net = 0 Policy net Loss = 178.287
epoch = 113 epsilon = 0.8845 retrain_policy net = 0 Policy net Loss = 180.162
epoch = 114 epsilon = 0.88445 retrain_policy net = 0 Policy net Loss = 183.602
epoch = 115 epsilon = 0.8844 retrain_policy net = 0 Policy net Loss = 155.905
epoch = 116 epsilon = 0.88435 retrain_policy net = 0 Policy net Loss = 172.379
epoch = 117 epsilon = 0.8843 retrain_policy net = 0 Policy net Loss = 217.063
epoch = 118 epsilon = 0.88425 retrain_policy net = 0 Policy net Loss = 208.438
epoch = 119 epsilon = 0.8842 retrain_policy net = 0 Policy net Loss = 172.184
epoch = 120 epsilon = 0.88415 retrain_policy net = 0 Policy net Loss = 157.811
epoch = 121 epsilon = 0.8841 retrain_policy net = 0 Policy net Loss = 176.183
epoch = 122 epsilon = 0.88405 retrain_policy net = 0 Policy net Loss = 157.017
epoch = 123 epsilon = 0.884 retrain_policy net = 0 Policy net Loss = 182.903
epoch = 124 epsilon = 0.88395 retrain_policy net = 0 Policy net Loss = 175.698
epoch = 125 epsilon = 0.8839 retrain_policy net = 0 Policy net Loss = 159.575
epoch = 126 epsilon = 0.88385 retrain_policy net = 0 Policy net Loss = 202.506
epoch = 127 epsilon = 0.8838 retrain_policy net = 0 Policy net Loss = 170.232
epoch = 128 epsilon = 0.88375 retrain_policy net = 0 Policy net Loss = 157.402
epoch = 129 epsilon = 0.8837 retrain_policy net = 0 Policy net Loss = 170.261
epoch = 130 epsilon = 0.88365 retrain_policy net = 0 Policy net Loss = 187.534
epoch = 131 epsilon = 0.8836 retrain_policy net = 0 Policy net Loss = 195.231
epoch = 132 epsilon = 0.88355 retrain_policy net = 0 Policy net Loss = 160.968
epoch = 133 epsilon = 0.8835 retrain_policy net = 0 Policy net Loss = 146.08
epoch = 134 epsilon = 0.88345 retrain_policy net = 0 Policy net Loss = 214.186
epoch = 135 epsilon = 0.8834 retrain_policy net = 0 Policy net Loss = 204.433
epoch = 136 epsilon = 0.88335 retrain_policy net = 0 Policy net Loss = 171.176
epoch = 137 epsilon = 0.8833 retrain_policy net = 0 Policy net Loss = 189.728
epoch = 138 epsilon = 0.88325 retrain_policy net = 0 Policy net Loss = 189.832
epoch = 139 epsilon = 0.8832 retrain_policy net = 0 Policy net Loss = 167.341
epoch = 140 epsilon = 0.88315 retrain_policy net = 0 Policy net Loss = 155.895
epoch = 141 epsilon = 0.8831 retrain_policy net = 0 Policy net Loss = 201.444
epoch = 142 epsilon = 0.88305 retrain_policy net = 0 Policy net Loss = 186.19
epoch = 143 epsilon = 0.883 retrain_policy net = 0 Policy net Loss = 198.984
epoch = 144 epsilon = 0.88295 retrain_policy net = 0 Policy net Loss = 183.47
epoch = 145 epsilon = 0.8829 retrain_policy net = 0 Policy net Loss = 175.021
epoch = 146 epsilon = 0.88285 retrain_policy net = 0 Policy net Loss = 170.065
epoch = 147 epsilon = 0.8828 retrain_policy net = 0 Policy net Loss = 202.911
epoch = 148 epsilon = 0.88275 retrain_policy net = 0 Policy net Loss = 197.71
epoch = 149 epsilon = 0.8827 retrain_policy net = 0 Policy net Loss = 204.317
epoch = 150 epsilon = 0.88265 retrain_policy net = 0 Policy net Loss = 190.034
epoch = 151 epsilon = 0.8826 retrain_policy net = 0 Policy net Loss = 170.819
epoch = 152 epsilon = 0.88255 retrain_policy net = 0 Policy net Loss = 176.834
epoch = 153 epsilon = 0.8825 retrain_policy net = 0 Policy net Loss = 157.76
epoch = 154 epsilon = 0.88245 retrain_policy net = 0 Policy net Loss = 172.347
epoch = 155 epsilon = 0.8824 retrain_policy net = 0 Policy net Loss = 184.404
epoch = 156 epsilon = 0.88235 retrain_policy net = 0 Policy net Loss = 215.825
epoch = 157 epsilon = 0.8823 retrain_policy net = 0 Policy net Loss = 194.409
epoch = 158 epsilon = 0.88225 retrain_policy net = 0 Policy net Loss = 173.989
epoch = 159 epsilon = 0.8822 retrain_policy net = 0 Policy net Loss = 186.395
epoch = 160 epsilon = 0.88215 retrain_policy net = 0 Policy net Loss = 143.137
epoch = 161 epsilon = 0.8821 retrain_policy net = 0 Policy net Loss = 177.861
epoch = 162 epsilon = 0.88205 retrain_policy net = 0 Policy net Loss = 159.476
epoch = 163 epsilon = 0.882 retrain_policy net = 0 Policy net Loss = 185.268
epoch = 164 epsilon = 0.88195 retrain_policy net = 0 Policy net Loss = 193.374
epoch = 165 epsilon = 0.8819 retrain_policy net = 0 Policy net Loss = 188.529
epoch = 166 epsilon = 0.88185 retrain_policy net = 0 Policy net Loss = 197.736
epoch = 167 epsilon = 0.8818 retrain_policy net = 0 Policy net Loss = 152.247
epoch = 168 epsilon = 0.88175 retrain_policy net = 0 Policy net Loss = 152.83
epoch = 169 epsilon = 0.8817 retrain_policy net = 0 Policy net Loss = 198.805
epoch = 170 epsilon = 0.88165 retrain_policy net = 0 Policy net Loss = 176.517
epoch = 171 epsilon = 0.8816 retrain_policy net = 0 Policy net Loss = 186.907
epoch = 172 epsilon = 0.88155 retrain_policy net = 0 Policy net Loss = 180.547
epoch = 173 epsilon = 0.8815 retrain_policy net = 0 Policy net Loss = 154.718
epoch = 174 epsilon = 0.88145 retrain_policy net = 0 Policy net Loss = 183.443
epoch = 175 epsilon = 0.8814 retrain_policy net = 0 Policy net Loss = 170.363
epoch = 176 epsilon = 0.88135 retrain_policy net = 0 Policy net Loss = 184.841
epoch = 177 epsilon = 0.8813 retrain_policy net = 0 Policy net Loss = 213.735
epoch = 178 epsilon = 0.88125 retrain_policy net = 0 Policy net Loss = 167.006
epoch = 179 epsilon = 0.8812 retrain_policy net = 0 Policy net Loss = 190.947
epoch = 180 epsilon = 0.88115 retrain_policy net = 0 Policy net Loss = 190.796
epoch = 181 epsilon = 0.8811 retrain_policy net = 0 Policy net Loss = 182.56
************************************************************************************************
Normal mode is selected by user Next scene network will be used with frame future f-2,f-1,f0,f+1 
skip_scene_predictor_only_for_benchmarking = 0
************************************************************************************************
Next f+2 predictor network as well on th way.. not finnish yet ,,,
Parmeters settings :
next_scene_hid_nodes_L1 = 200
next_scene_hid_nodes_L2 = 200
next_scene_hid_nodes_L3 = 200
next_F2_scene_hid_nodes_L1 = 200
next_F2_scene_hid_nodes_L2 = 200
next_F2_scene_hid_nodes_L3 = 200
policy_net_hid_nodes_L1 = 85
policy_net_hid_nodes_L2 = 25
policy_net_hid_nodes_L3 = 10
next_scene_fc_net.learning_rate = 0.001
next_scene_fc_net.momentum = 0.1
next_F2_scene_fc_net.learning_rate = 0.001
next_F2_scene_fc_net.momentum = 0.1
policy_fc_net.learning_rate = 0.001
policy_fc_net.momentum = 0.2
g_replay_size = 20
derating_epsilon = 5e-05
gamma_decay = 0.85
target_policy_on_next_action_selection = 1
target_policy_use_f_p_2_pixel_from_predict_net = 1
target_off_set_to_output_itself = 1
skip_traning_next_scenen = 0
policy_fc_net.use_dropouts =0
policy_fc_net.dropout_proportion = 0
retrain_policy_net = 1
************************************************************************************************
Normal mode is selected by user Next scene network will be used with frame future f-2,f-1,f0,f+1 
skip_scene_predictor_only_for_benchmarking = 0
************************************************************************************************
Next f+2 predictor network as well on th way.. not finnish yet ,,,
Parmeters settings :
next_scene_hid_nodes_L1 = 200
next_scene_hid_nodes_L2 = 200
next_scene_hid_nodes_L3 = 200
next_F2_scene_hid_nodes_L1 = 200
next_F2_scene_hid_nodes_L2 = 200
next_F2_scene_hid_nodes_L3 = 200
policy_net_hid_nodes_L1 = 85
policy_net_hid_nodes_L2 = 25
policy_net_hid_nodes_L3 = 10
next_scene_fc_net.learning_rate = 0.001
next_scene_fc_net.momentum = 0.1
next_F2_scene_fc_net.learning_rate = 0.001
next_F2_scene_fc_net.momentum = 0.1
policy_fc_net.learning_rate = 0.001
policy_fc_net.momentum = 0.2
g_replay_size = 20
derating_epsilon = 5e-05
gamma_decay = 0.85
target_policy_on_next_action_selection = 1
target_policy_use_f_p_2_pixel_from_predict_net = 1
target_off_set_to_output_itself = 1
skip_traning_next_scenen = 0
policy_fc_net.use_dropouts =0
policy_fc_net.dropout_proportion = 0
retrain_policy_net = 1
epoch = 1 epsilon = 0.93 retrain_policy net = 0 Policy net Loss = 171.877
epoch = 2 epsilon = 0.91 retrain_policy net = 0 Policy net Loss = 181.886
epoch = 3 epsilon = 0.89 retrain_policy net = 0 Policy net Loss = 184.284
epoch = 4 epsilon = 0.88995 retrain_policy net = 0 Policy net Loss = 231.455
epoch = 5 epsilon = 0.8899 retrain_policy net = 0 Policy net Loss = 195.464
epoch = 6 epsilon = 0.88985 retrain_policy net = 0 Policy net Loss = 207.576
epoch = 7 epsilon = 0.8898 retrain_policy net = 0 Policy net Loss = 182.74
epoch = 8 epsilon = 0.88975 retrain_policy net = 0 Policy net Loss = 154.886
epoch = 9 epsilon = 0.8897 retrain_policy net = 0 Policy net Loss = 182.62
epoch = 10 epsilon = 0.88965 retrain_policy net = 0 Policy net Loss = 174.8
epoch = 11 epsilon = 0.8896 retrain_policy net = 0 Policy net Loss = 170.228
epoch = 12 epsilon = 0.88955 retrain_policy net = 0 Policy net Loss = 197.715
epoch = 13 epsilon = 0.8895 retrain_policy net = 0 Policy net Loss = 163.206
epoch = 14 epsilon = 0.88945 retrain_policy net = 0 Policy net Loss = 207.084
epoch = 15 epsilon = 0.8894 retrain_policy net = 0 Policy net Loss = 182.437
epoch = 16 epsilon = 0.88935 retrain_policy net = 0 Policy net Loss = 203.83
epoch = 17 epsilon = 0.8893 retrain_policy net = 0 Policy net Loss = 158.048
epoch = 18 epsilon = 0.88925 retrain_policy net = 0 Policy net Loss = 185.589
epoch = 19 epsilon = 0.8892 retrain_policy net = 0 Policy net Loss = 171.975
epoch = 20 epsilon = 0.88915 retrain_policy net = 0 Policy net Loss = 165.099
epoch = 21 epsilon = 0.8891 retrain_policy net = 0 Policy net Loss = 184.591
epoch = 22 epsilon = 0.88905 retrain_policy net = 0 Policy net Loss = 161.703
epoch = 23 epsilon = 0.889 retrain_policy net = 0 Policy net Loss = 178.623
epoch = 24 epsilon = 0.88895 retrain_policy net = 0 Policy net Loss = 186.137
epoch = 25 epsilon = 0.8889 retrain_policy net = 0 Policy net Loss = 149.683
epoch = 26 epsilon = 0.88885 retrain_policy net = 0 Policy net Loss = 191.885
epoch = 27 epsilon = 0.8888 retrain_policy net = 0 Policy net Loss = 156.248
epoch = 28 epsilon = 0.88875 retrain_policy net = 0 Policy net Loss = 196.085
epoch = 29 epsilon = 0.8887 retrain_policy net = 0 Policy net Loss = 197.785
epoch = 30 epsilon = 0.88865 retrain_policy net = 0 Policy net Loss = 209.696
epoch = 31 epsilon = 0.8886 retrain_policy net = 0 Policy net Loss = 175.79
epoch = 32 epsilon = 0.88855 retrain_policy net = 0 Policy net Loss = 176.01
epoch = 33 epsilon = 0.8885 retrain_policy net = 0 Policy net Loss = 171.839
epoch = 34 epsilon = 0.88845 retrain_policy net = 0 Policy net Loss = 198.439
epoch = 35 epsilon = 0.8884 retrain_policy net = 0 Policy net Loss = 204.259
epoch = 36 epsilon = 0.88835 retrain_policy net = 0 Policy net Loss = 199.175
epoch = 37 epsilon = 0.8883 retrain_policy net = 0 Policy net Loss = 201.356
epoch = 38 epsilon = 0.88825 retrain_policy net = 0 Policy net Loss = 178.236
epoch = 39 epsilon = 0.8882 retrain_policy net = 0 Policy net Loss = 174.784
epoch = 40 epsilon = 0.88815 retrain_policy net = 0 Policy net Loss = 175.309
epoch = 41 epsilon = 0.8881 retrain_policy net = 0 Policy net Loss = 189.534
epoch = 42 epsilon = 0.88805 retrain_policy net = 0 Policy net Loss = 185.065
epoch = 43 epsilon = 0.888 retrain_policy net = 0 Policy net Loss = 183.677
epoch = 44 epsilon = 0.88795 retrain_policy net = 0 Policy net Loss = 165.11
epoch = 45 epsilon = 0.8879 retrain_policy net = 0 Policy net Loss = 181.016
epoch = 46 epsilon = 0.88785 retrain_policy net = 0 Policy net Loss = 217.799
epoch = 47 epsilon = 0.8878 retrain_policy net = 0 Policy net Loss = 189.325
epoch = 48 epsilon = 0.88775 retrain_policy net = 0 Policy net Loss = 185.145
epoch = 49 epsilon = 0.8877 retrain_policy net = 0 Policy net Loss = 189.517
epoch = 50 epsilon = 0.88765 retrain_policy net = 0 Policy net Loss = 203.183
epoch = 51 epsilon = 0.8876 retrain_policy net = 0 Policy net Loss = 209.883
epoch = 52 epsilon = 0.88755 retrain_policy net = 0 Policy net Loss = 183.277
epoch = 53 epsilon = 0.8875 retrain_policy net = 0 Policy net Loss = 197.167
epoch = 54 epsilon = 0.88745 retrain_policy net = 0 Policy net Loss = 193.644
epoch = 55 epsilon = 0.8874 retrain_policy net = 0 Policy net Loss = 209.602
epoch = 56 epsilon = 0.88735 retrain_policy net = 0 Policy net Loss = 169.103
epoch = 57 epsilon = 0.8873 retrain_policy net = 0 Policy net Loss = 190.997
epoch = 58 epsilon = 0.88725 retrain_policy net = 0 Policy net Loss = 197.929
epoch = 59 epsilon = 0.8872 retrain_policy net = 0 Policy net Loss = 175.237
epoch = 60 epsilon = 0.88715 retrain_policy net = 0 Policy net Loss = 146.59
epoch = 61 epsilon = 0.8871 retrain_policy net = 0 Policy net Loss = 154.379
epoch = 62 epsilon = 0.88705 retrain_policy net = 0 Policy net Loss = 177.237
epoch = 63 epsilon = 0.887 retrain_policy net = 0 Policy net Loss = 203.959
epoch = 64 epsilon = 0.88695 retrain_policy net = 0 Policy net Loss = 185.504
epoch = 65 epsilon = 0.8869 retrain_policy net = 0 Policy net Loss = 212.537
epoch = 66 epsilon = 0.88685 retrain_policy net = 0 Policy net Loss = 183.131
epoch = 67 epsilon = 0.8868 retrain_policy net = 0 Policy net Loss = 178.114
epoch = 68 epsilon = 0.88675 retrain_policy net = 0 Policy net Loss = 211.26
epoch = 69 epsilon = 0.8867 retrain_policy net = 0 Policy net Loss = 196.276
epoch = 70 epsilon = 0.88665 retrain_policy net = 0 Policy net Loss = 189.797
epoch = 71 epsilon = 0.8866 retrain_policy net = 0 Policy net Loss = 157.203
epoch = 72 epsilon = 0.88655 retrain_policy net = 0 Policy net Loss = 130.476
epoch = 73 epsilon = 0.8865 retrain_policy net = 0 Policy net Loss = 233.42
epoch = 74 epsilon = 0.88645 retrain_policy net = 0 Policy net Loss = 185.739
epoch = 75 epsilon = 0.8864 retrain_policy net = 0 Policy net Loss = 166.095
epoch = 76 epsilon = 0.88635 retrain_policy net = 0 Policy net Loss = 209.179
epoch = 77 epsilon = 0.8863 retrain_policy net = 0 Policy net Loss = 165.813
epoch = 78 epsilon = 0.88625 retrain_policy net = 0 Policy net Loss = 195.461
epoch = 79 epsilon = 0.8862 retrain_policy net = 0 Policy net Loss = 184.692
epoch = 80 epsilon = 0.88615 retrain_policy net = 0 Policy net Loss = 187.263
epoch = 81 epsilon = 0.8861 retrain_policy net = 0 Policy net Loss = 193.964
epoch = 82 epsilon = 0.88605 retrain_policy net = 0 Policy net Loss = 202.082
epoch = 83 epsilon = 0.886 retrain_policy net = 0 Policy net Loss = 188.302
epoch = 84 epsilon = 0.88595 retrain_policy net = 0 Policy net Loss = 202.772
epoch = 85 epsilon = 0.8859 retrain_policy net = 0 Policy net Loss = 205.209
epoch = 86 epsilon = 0.88585 retrain_policy net = 0 Policy net Loss = 158.269
epoch = 87 epsilon = 0.8858 retrain_policy net = 0 Policy net Loss = 135.485
epoch = 88 epsilon = 0.88575 retrain_policy net = 0 Policy net Loss = 215.686
epoch = 89 epsilon = 0.8857 retrain_policy net = 0 Policy net Loss = 158.312
epoch = 90 epsilon = 0.88565 retrain_policy net = 0 Policy net Loss = 160.032
epoch = 91 epsilon = 0.8856 retrain_policy net = 0 Policy net Loss = 203.119
epoch = 92 epsilon = 0.88555 retrain_policy net = 0 Policy net Loss = 170.326
epoch = 93 epsilon = 0.8855 retrain_policy net = 0 Policy net Loss = 184.999
epoch = 94 epsilon = 0.88545 retrain_policy net = 0 Policy net Loss = 165.664
epoch = 95 epsilon = 0.8854 retrain_policy net = 0 Policy net Loss = 188.445
epoch = 96 epsilon = 0.88535 retrain_policy net = 0 Policy net Loss = 185.513
epoch = 97 epsilon = 0.8853 retrain_policy net = 0 Policy net Loss = 200.847
epoch = 98 epsilon = 0.88525 retrain_policy net = 0 Policy net Loss = 174.902
epoch = 99 epsilon = 0.8852 retrain_policy net = 0 Policy net Loss = 181.136
epoch = 100 epsilon = 0.88515 retrain_policy net = 0 Policy net Loss = 141.659
epoch = 101 epsilon = 0.8851 retrain_policy net = 0 Policy net Loss = 162.398
epoch = 102 epsilon = 0.88505 retrain_policy net = 0 Policy net Loss = 145.117
epoch = 103 epsilon = 0.885 retrain_policy net = 0 Policy net Loss = 177.412
epoch = 104 epsilon = 0.88495 retrain_policy net = 0 Policy net Loss = 217.349
epoch = 105 epsilon = 0.8849 retrain_policy net = 0 Policy net Loss = 200.774
epoch = 106 epsilon = 0.88485 retrain_policy net = 0 Policy net Loss = 204.184
epoch = 107 epsilon = 0.8848 retrain_policy net = 0 Policy net Loss = 193.559
epoch = 108 epsilon = 0.88475 retrain_policy net = 0 Policy net Loss = 143.221
epoch = 109 epsilon = 0.8847 retrain_policy net = 0 Policy net Loss = 169.121
epoch = 110 epsilon = 0.88465 retrain_policy net = 0 Policy net Loss = 176.456
epoch = 111 epsilon = 0.8846 retrain_policy net = 0 Policy net Loss = 177.008
epoch = 112 epsilon = 0.88455 retrain_policy net = 0 Policy net Loss = 149.819
epoch = 113 epsilon = 0.8845 retrain_policy net = 0 Policy net Loss = 188.297
epoch = 114 epsilon = 0.88445 retrain_policy net = 0 Policy net Loss = 182.007
epoch = 115 epsilon = 0.8844 retrain_policy net = 0 Policy net Loss = 158.165
epoch = 116 epsilon = 0.88435 retrain_policy net = 0 Policy net Loss = 183.414
epoch = 117 epsilon = 0.8843 retrain_policy net = 0 Policy net Loss = 146.05
epoch = 118 epsilon = 0.88425 retrain_policy net = 0 Policy net Loss = 150.207
epoch = 119 epsilon = 0.8842 retrain_policy net = 0 Policy net Loss = 155.981
epoch = 120 epsilon = 0.88415 retrain_policy net = 0 Policy net Loss = 192.95
epoch = 121 epsilon = 0.8841 retrain_policy net = 0 Policy net Loss = 190.04
epoch = 122 epsilon = 0.88405 retrain_policy net = 0 Policy net Loss = 178.052
epoch = 123 epsilon = 0.884 retrain_policy net = 0 Policy net Loss = 187.655
epoch = 124 epsilon = 0.88395 retrain_policy net = 0 Policy net Loss = 187.849
epoch = 125 epsilon = 0.8839 retrain_policy net = 0 Policy net Loss = 192.89
epoch = 126 epsilon = 0.88385 retrain_policy net = 0 Policy net Loss = 173.053
epoch = 127 epsilon = 0.8838 retrain_policy net = 0 Policy net Loss = 198.925
epoch = 128 epsilon = 0.88375 retrain_policy net = 0 Policy net Loss = 165.716
epoch = 129 epsilon = 0.8837 retrain_policy net = 0 Policy net Loss = 168.604
epoch = 130 epsilon = 0.88365 retrain_policy net = 0 Policy net Loss = 186.431
epoch = 131 epsilon = 0.8836 retrain_policy net = 0 Policy net Loss = 157.847
epoch = 132 epsilon = 0.88355 retrain_policy net = 0 Policy net Loss = 201.974
epoch = 133 epsilon = 0.8835 retrain_policy net = 0 Policy net Loss = 174.206
epoch = 134 epsilon = 0.88345 retrain_policy net = 0 Policy net Loss = 156.187
epoch = 135 epsilon = 0.8834 retrain_policy net = 0 Policy net Loss = 204.29
epoch = 136 epsilon = 0.88335 retrain_policy net = 0 Policy net Loss = 149.29
epoch = 137 epsilon = 0.8833 retrain_policy net = 0 Policy net Loss = 201.466
epoch = 138 epsilon = 0.88325 retrain_policy net = 0 Policy net Loss = 194.085
epoch = 139 epsilon = 0.8832 retrain_policy net = 0 Policy net Loss = 144.037
epoch = 140 epsilon = 0.88315 retrain_policy net = 0 Policy net Loss = 179.69
epoch = 141 epsilon = 0.8831 retrain_policy net = 0 Policy net Loss = 166.505
epoch = 142 epsilon = 0.88305 retrain_policy net = 0 Policy net Loss = 159.085
epoch = 143 epsilon = 0.883 retrain_policy net = 0 Policy net Loss = 200.447
epoch = 144 epsilon = 0.88295 retrain_policy net = 0 Policy net Loss = 143.181
epoch = 145 epsilon = 0.8829 retrain_policy net = 0 Policy net Loss = 186.484
epoch = 146 epsilon = 0.88285 retrain_policy net = 0 Policy net Loss = 168.493
epoch = 147 epsilon = 0.8828 retrain_policy net = 0 Policy net Loss = 201.441
epoch = 148 epsilon = 0.88275 retrain_policy net = 0 Policy net Loss = 186.43
epoch = 149 epsilon = 0.8827 retrain_policy net = 0 Policy net Loss = 174.624
epoch = 150 epsilon = 0.88265 retrain_policy net = 0 Policy net Loss = 156.044
epoch = 151 epsilon = 0.8826 retrain_policy net = 0 Policy net Loss = 150.468
epoch = 152 epsilon = 0.88255 retrain_policy net = 0 Policy net Loss = 191.702
epoch = 153 epsilon = 0.8825 retrain_policy net = 0 Policy net Loss = 209.516
epoch = 154 epsilon = 0.88245 retrain_policy net = 0 Policy net Loss = 182.035
epoch = 155 epsilon = 0.8824 retrain_policy net = 0 Policy net Loss = 159.257
epoch = 156 epsilon = 0.88235 retrain_policy net = 0 Policy net Loss = 170.631
epoch = 157 epsilon = 0.8823 retrain_policy net = 0 Policy net Loss = 173.649
epoch = 158 epsilon = 0.88225 retrain_policy net = 0 Policy net Loss = 163.108
epoch = 159 epsilon = 0.8822 retrain_policy net = 0 Policy net Loss = 187.56
epoch = 160 epsilon = 0.88215 retrain_policy net = 0 Policy net Loss = 153.432
epoch = 161 epsilon = 0.8821 retrain_policy net = 0 Policy net Loss = 171.504
epoch = 162 epsilon = 0.88205 retrain_policy net = 0 Policy net Loss = 156.618
epoch = 163 epsilon = 0.882 retrain_policy net = 0 Policy net Loss = 193.05
epoch = 164 epsilon = 0.88195 retrain_policy net = 0 Policy net Loss = 178.29
epoch = 165 epsilon = 0.8819 retrain_policy net = 0 Policy net Loss = 184.289
epoch = 166 epsilon = 0.88185 retrain_policy net = 0 Policy net Loss = 189.564
epoch = 167 epsilon = 0.8818 retrain_policy net = 0 Policy net Loss = 207.84
epoch = 168 epsilon = 0.88175 retrain_policy net = 0 Policy net Loss = 158.019
epoch = 169 epsilon = 0.8817 retrain_policy net = 0 Policy net Loss = 138.465
epoch = 170 epsilon = 0.88165 retrain_policy net = 0 Policy net Loss = 163.573
epoch = 171 epsilon = 0.8816 retrain_policy net = 0 Policy net Loss = 136.156
epoch = 172 epsilon = 0.88155 retrain_policy net = 0 Policy net Loss = 178.698
epoch = 173 epsilon = 0.8815 retrain_policy net = 0 Policy net Loss = 166.959
epoch = 174 epsilon = 0.88145 retrain_policy net = 0 Policy net Loss = 153.03
epoch = 175 epsilon = 0.8814 retrain_policy net = 0 Policy net Loss = 188.876
epoch = 176 epsilon = 0.88135 retrain_policy net = 0 Policy net Loss = 183.329
epoch = 177 epsilon = 0.8813 retrain_policy net = 0 Policy net Loss = 202.559
epoch = 178 epsilon = 0.88125 retrain_policy net = 0 Policy net Loss = 187.276
epoch = 179 epsilon = 0.8812 retrain_policy net = 0 Policy net Loss = 168.369
epoch = 180 epsilon = 0.88115 retrain_policy net = 0 Policy net Loss = 167.668
epoch = 181 epsilon = 0.8811 retrain_policy net = 0 Policy net Loss = 197.689
epoch = 182 epsilon = 0.88105 retrain_policy net = 0 Policy net Loss = 183.713
epoch = 183 epsilon = 0.881 retrain_policy net = 0 Policy net Loss = 157.424
epoch = 184 epsilon = 0.88095 retrain_policy net = 0 Policy net Loss = 196.19
epoch = 185 epsilon = 0.8809 retrain_policy net = 0 Policy net Loss = 162.967
epoch = 186 epsilon = 0.88085 retrain_policy net = 0 Policy net Loss = 166.798
epoch = 187 epsilon = 0.8808 retrain_policy net = 0 Policy net Loss = 149.897
epoch = 188 epsilon = 0.88075 retrain_policy net = 0 Policy net Loss = 173.025
epoch = 189 epsilon = 0.8807 retrain_policy net = 0 Policy net Loss = 195.664
epoch = 190 epsilon = 0.88065 retrain_policy net = 0 Policy net Loss = 148.286
epoch = 191 epsilon = 0.8806 retrain_policy net = 0 Policy net Loss = 212.799
epoch = 192 epsilon = 0.88055 retrain_policy net = 0 Policy net Loss = 162.117
epoch = 193 epsilon = 0.8805 retrain_policy net = 0 Policy net Loss = 206.755
epoch = 194 epsilon = 0.88045 retrain_policy net = 0 Policy net Loss = 171.331
epoch = 195 epsilon = 0.8804 retrain_policy net = 0 Policy net Loss = 188.62
epoch = 196 epsilon = 0.88035 retrain_policy net = 0 Policy net Loss = 178.007
epoch = 197 epsilon = 0.8803 retrain_policy net = 0 Policy net Loss = 140.982
epoch = 198 epsilon = 0.88025 retrain_policy net = 0 Policy net Loss = 191.648
epoch = 199 epsilon = 0.8802 retrain_policy net = 0 Policy net Loss = 172.878
epoch = 200 epsilon = 0.88015 retrain_policy net = 0 Policy net Loss = 155.342
epoch = 201 epsilon = 0.8801 retrain_policy net = 0 Policy net Loss = 118.19
epoch = 202 epsilon = 0.88005 retrain_policy net = 0 Policy net Loss = 159.049
epoch = 203 epsilon = 0.88 retrain_policy net = 0 Policy net Loss = 164.93
epoch = 204 epsilon = 0.87995 retrain_policy net = 0 Policy net Loss = 183.415
epoch = 205 epsilon = 0.8799 retrain_policy net = 0 Policy net Loss = 171.17
epoch = 206 epsilon = 0.87985 retrain_policy net = 0 Policy net Loss = 157.488
epoch = 207 epsilon = 0.8798 retrain_policy net = 0 Policy net Loss = 153.142
epoch = 208 epsilon = 0.87975 retrain_policy net = 0 Policy net Loss = 170.777
epoch = 209 epsilon = 0.8797 retrain_policy net = 0 Policy net Loss = 178.192
epoch = 210 epsilon = 0.87965 retrain_policy net = 0 Policy net Loss = 176.333
epoch = 211 epsilon = 0.8796 retrain_policy net = 0 Policy net Loss = 164.877
epoch = 212 epsilon = 0.87955 retrain_policy net = 0 Policy net Loss = 178.117
epoch = 213 epsilon = 0.8795 retrain_policy net = 0 Policy net Loss = 159.144
epoch = 214 epsilon = 0.87945 retrain_policy net = 0 Policy net Loss = 184.389
epoch = 215 epsilon = 0.8794 retrain_policy net = 0 Policy net Loss = 213.602
epoch = 216 epsilon = 0.87935 retrain_policy net = 0 Policy net Loss = 167.119
epoch = 217 epsilon = 0.8793 retrain_policy net = 0 Policy net Loss = 127.799
epoch = 218 epsilon = 0.87925 retrain_policy net = 0 Policy net Loss = 213.688
epoch = 219 epsilon = 0.8792 retrain_policy net = 0 Policy net Loss = 189.607
epoch = 220 epsilon = 0.87915 retrain_policy net = 0 Policy net Loss = 200.092
epoch = 221 epsilon = 0.8791 retrain_policy net = 0 Policy net Loss = 167.974
epoch = 222 epsilon = 0.87905 retrain_policy net = 0 Policy net Loss = 180.976
************************************************************************************************
Normal mode is selected by user Next scene network will be used with frame future f-2,f-1,f0,f+1 
skip_scene_predictor_only_for_benchmarking = 0
************************************************************************************************
Next f+2 predictor network as well on th way.. not finnish yet ,,,
Parmeters settings :
next_scene_hid_nodes_L1 = 200
next_scene_hid_nodes_L2 = 200
next_scene_hid_nodes_L3 = 200
next_F2_scene_hid_nodes_L1 = 200
next_F2_scene_hid_nodes_L2 = 200
next_F2_scene_hid_nodes_L3 = 200
policy_net_hid_nodes_L1 = 85
policy_net_hid_nodes_L2 = 25
policy_net_hid_nodes_L3 = 10
next_scene_fc_net.learning_rate = 0.001
next_scene_fc_net.momentum = 0.1
next_F2_scene_fc_net.learning_rate = 0.001
next_F2_scene_fc_net.momentum = 0.1
policy_fc_net.learning_rate = 0.001
policy_fc_net.momentum = 0.2
g_replay_size = 20
derating_epsilon = 5e-05
gamma_decay = 0.85
target_policy_on_next_action_selection = 1
target_policy_use_f_p_2_pixel_from_predict_net = 1
target_off_set_to_output_itself = 1
skip_traning_next_scenen = 0
policy_fc_net.use_dropouts =0
policy_fc_net.dropout_proportion = 0
retrain_policy_net = 1
epoch = 1 epsilon = 0.93 retrain_policy net = 0 Policy net Loss = 153.652
************************************************************************************************
Normal mode is selected by user Next scene network will be used with frame future f-2,f-1,f0,f+1 
skip_scene_predictor_only_for_benchmarking = 0
************************************************************************************************
Next f+2 predictor network as well on th way.. not finnish yet ,,,
Parmeters settings :
next_scene_hid_nodes_L1 = 200
next_scene_hid_nodes_L2 = 200
next_scene_hid_nodes_L3 = 200
next_F2_scene_hid_nodes_L1 = 200
next_F2_scene_hid_nodes_L2 = 200
next_F2_scene_hid_nodes_L3 = 200
policy_net_hid_nodes_L1 = 85
policy_net_hid_nodes_L2 = 25
policy_net_hid_nodes_L3 = 10
next_scene_fc_net.learning_rate = 0.001
next_scene_fc_net.momentum = 0.1
next_F2_scene_fc_net.learning_rate = 0.001
next_F2_scene_fc_net.momentum = 0.1
policy_fc_net.learning_rate = 0.001
policy_fc_net.momentum = 0.2
g_replay_size = 20
derating_epsilon = 5e-05
gamma_decay = 0.85
target_policy_on_next_action_selection = 1
target_policy_use_f_p_2_pixel_from_predict_net = 1
target_off_set_to_output_itself = 1
skip_traning_next_scenen = 0
policy_fc_net.use_dropouts =0
policy_fc_net.dropout_proportion = 0
retrain_policy_net = 1
epoch = 1 epsilon = 0.93 retrain_policy net = 0 Policy net Loss = 167.693
